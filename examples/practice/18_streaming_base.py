
import sys
import os
import time
from dotenv import load_dotenv, dotenv_values
from ai_toolkit.models import ModelManager
from pydantic import BaseModel
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime
from langchain_core.messages import HumanMessage

load_dotenv()
# Create model manager
manager = ModelManager()

# Create a model (auto-loads API key from environment - recommended)
model = manager.create_model(
    api_key=os.environ.get('QWEN_API_KEY'),
    provider="qwen",           # Provider: deepseek, qwen, or glm
    model="qwen-turbo",         # Model variant (optional)
    temperature=0.7,               # Randomness: 0.0-2.0
    max_tokens=2000                # Max response length
)


from langchain.agents import create_agent


# 1.Agent progress
def get_weather(city: str) -> str:
    """Get weather for a given city."""

    return f"It's always sunny in {city}!"

agent = create_agent(
    model=model,
    tools=[get_weather],
)

for chunk in agent.stream(  
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="updates",
):
    for step, data in chunk.items():
        print(f"step: {step}")
        print(f"content: {data['messages'][-1].content_blocks}")


# 2.LLM tokens
def get_weather(city: str) -> str:
    """Get weather for a given city."""

    return f"It's always sunny in {city}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
)
for token, metadata in agent.stream(  
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="messages",
):
    print(f"node: {metadata['langgraph_node']}")
    print(f"content: {token.content_blocks}")
    print("\n")


# 3.Custom updates
from langchain.agents import create_agent
from langgraph.config import get_stream_writer  


def get_weather(city: str) -> str:
    """Get weather for a given city."""
    writer = get_stream_writer()  
    # stream any arbitrary data
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")
    return f"It's always sunny in {city}!"

agent = create_agent(
    model=model,
    tools=[get_weather],
)

for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="custom"
):
    print(chunk)    


# 4.Stream multiple modes
from langchain.agents import create_agent
from langgraph.config import get_stream_writer


def get_weather(city: str) -> str:
    """Get weather for a given city."""
    writer = get_stream_writer()
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")
    return f"It's always sunny in {city}!"

agent = create_agent(
    model=model,
    tools=[get_weather],
)

for stream_mode, chunk in agent.stream(  
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode=["updates", "custom"]
):
    print(f"stream_mode: {stream_mode}")
    print(f"content: {chunk}")
    print("\n")
    

# 5.Common patterns

## 5.1 Streaming tool calls
from typing import Any

from langchain.agents import create_agent
from langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage


def get_weather(city: str) -> str:
    """Get weather for a given city."""

    return f"It's always sunny in {city}!"


agent = create_agent(model, tools=[get_weather])


def _render_message_chunk(token: AIMessageChunk) -> None:
    # print("token:", token)
    if token.text:
        print(token.text, end="|")
    if token.tool_call_chunks:
        print(token.tool_call_chunks)
    # N.B. all content is available through token.content_blocks


def _render_completed_message(message: AnyMessage) -> None:
    if isinstance(message, AIMessage) and message.tool_calls:
        print(f"Tool calls: {message.tool_calls}")
    if isinstance(message, ToolMessage):
        print(f"Tool response: {message.content_blocks}")


input_message = {"role": "user", "content": "What is the weather in Boston?"}
for stream_mode, data in agent.stream(
    {"messages": [input_message]},
    stream_mode=["messages", "updates"],  
):
    if stream_mode == "messages":
        token, metadata = data
        if isinstance(token, AIMessageChunk):
            _render_message_chunk(token)  
    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):  # `source` captures node name
                _render_completed_message(update["messages"][-1])


## 5.2 Streaming with human-in-the-loop
from typing import Any

from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage
from langchain_openai import ChatOpenAI  # è¡¥å……æ¨¡å‹åˆå§‹åŒ–ä¾èµ–
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import Command, Interrupt


# å®šä¹‰å¤©æ°”å·¥å…·
def get_weather(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"


# åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ›¿æ¢ä¸ºä½ è‡ªå·±çš„æ¨¡å‹é…ç½®ï¼Œè¿™é‡Œç”¨OpenAIç¤ºä¾‹ï¼‰
model = ChatOpenAI(
    model="gpt-4o",  # å¯æ›¿æ¢ä¸ºå…¶ä»–æ¨¡å‹
    api_key="your-openai-api-key",  # æ›¿æ¢ä¸ºä½ çš„API Key
    streaming=True  # å¼€å¯æµå¼è¾“å‡º
)

# åˆå§‹åŒ–æ£€æŸ¥ç‚¹ï¼ˆä¿å­˜ä¼šè¯çŠ¶æ€ï¼Œæ”¯æŒä¸­æ–­æ¢å¤ï¼‰
checkpointer = InMemorySaver()

# åˆ›å»ºå¸¦äººæœºååŒä¸­é—´ä»¶çš„æ™ºèƒ½ä½“
agent = create_agent(
    model,
    tools=[get_weather],
    middleware=[
        # é…ç½®ï¼šè°ƒç”¨get_weatherå·¥å…·æ—¶è§¦å‘äººå·¥ä¸­æ–­
        HumanInTheLoopMiddleware(interrupt_on={"get_weather": True}),
    ],
    checkpointer=checkpointer,
)


# è‡ªå®šä¹‰æ¸²æŸ“å‡½æ•°ï¼šå¤„ç†LLMç”Ÿæˆçš„æ¶ˆæ¯ç‰‡æ®µ
def _render_message_chunk(token: AIMessageChunk) -> None:
    if token.text:
        print(token.text, end="|", flush=True)  # flush=Trueç¡®ä¿å®æ—¶æ‰“å°
    if token.tool_call_chunks:
        print(f"\nå·¥å…·è°ƒç”¨ç‰‡æ®µ: {token.tool_call_chunks}")


# è‡ªå®šä¹‰æ¸²æŸ“å‡½æ•°ï¼šå¤„ç†å®Œæˆçš„æ¶ˆæ¯ï¼ˆå·¥å…·è°ƒç”¨/å·¥å…·è¿”å›ï¼‰
def _render_completed_message(message: AnyMessage) -> None:
    if isinstance(message, AIMessage) and message.tool_calls:
        print(f"\nå®Œæ•´å·¥å…·è°ƒç”¨æŒ‡ä»¤: {message.tool_calls}")
    if isinstance(message, ToolMessage):
        print(f"\nå·¥å…·è¿”å›ç»“æœ: {message.content_blocks}")


# è‡ªå®šä¹‰æ¸²æŸ“å‡½æ•°ï¼šå¤„ç†äººå·¥ä¸­æ–­æç¤ºï¼Œå¹¶ç­‰å¾…äººå·¥å®¡æ‰¹
def _handle_interrupt(interrupt: Interrupt) -> bool:
    """
    å¤„ç†äººå·¥ä¸­æ–­ï¼Œè¿”å›æ˜¯å¦åŒæ„ç»§ç»­æ‰§è¡Œ
    :param interrupt: ä¸­æ–­å¯¹è±¡
    :return: True=åŒæ„ï¼ŒFalse=æ‹’ç»
    """
    interrupts = interrupt.value
    print("\n===== è§¦å‘äººå·¥å®¡æ‰¹ =====")
    for request in interrupts["action_requests"]:
        print(f"å®¡æ‰¹æç¤º: {request['description']}")
    
    # ç­‰å¾…äººå·¥è¾“å…¥å®¡æ‰¹ç»“æœ
    while True:
        human_input = input("æ˜¯å¦åŒæ„è°ƒç”¨get_weatherå·¥å…·ï¼Ÿ(è¾“å…¥yåŒæ„/è¾“å…¥næ‹’ç»): ").strip().lower()
        if human_input in ["y", "n"]:
            break
        print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥yæˆ–nï¼")
    
    if human_input == "y":
        print("âœ… äººå·¥å®¡æ‰¹é€šè¿‡ï¼Œç»§ç»­æ‰§è¡Œå·¥å…·è°ƒç”¨...")
        return True
    else:
        print("âŒ äººå·¥å®¡æ‰¹æ‹’ç»ï¼Œç»ˆæ­¢æµç¨‹ï¼")
        return False


# ä¸»æ‰§è¡Œé€»è¾‘
if __name__ == "__main__":
    # ç”¨æˆ·è¾“å…¥ï¼šæŸ¥è¯¢æ³¢å£«é¡¿å’Œæ—§é‡‘å±±å¤©æ°”
    input_message = {
        "role": "user",
        "content": "Can you look up the weather in Boston and San Francisco?"
    }

    # é…ç½®ä¼šè¯IDï¼šä¿è¯ä¸­æ–­åçŠ¶æ€ä¸€è‡´
    config = {"configurable": {"thread_id": "weather_query_123"}}
    interrupts = []
    approval_granted = True  # æ ‡è®°æ˜¯å¦é€šè¿‡äººå·¥å®¡æ‰¹

    # æµå¼æ‰§è¡Œæ™ºèƒ½ä½“
    for stream_mode, data in agent.stream(
        {"messages": [input_message]},
        config=config,
        stream_mode=["messages", "updates"],
    ):
        # è‹¥å·²æ‹’ç»å®¡æ‰¹ï¼Œç›´æ¥ç»ˆæ­¢å¾ªç¯
        if not approval_granted:
            break

        # å¤„ç†messagesæ¨¡å¼ï¼šå®æ—¶æ‰“å°LLMç”Ÿæˆç‰‡æ®µ
        if stream_mode == "messages":
            token, metadata = data
            if isinstance(token, AIMessageChunk):
                _render_message_chunk(token)

        # å¤„ç†updatesæ¨¡å¼ï¼šå®Œæ•´èŠ‚ç‚¹ï¼ˆæ¨¡å‹/å·¥å…·/ä¸­æ–­ï¼‰
        if stream_mode == "updates":
            for source, update in data.items():
                # å¤„ç†æ¨¡å‹/å·¥å…·èŠ‚ç‚¹çš„å®Œæˆæ¶ˆæ¯
                if source in ("model", "tools"):
                    _render_completed_message(update["messages"][-1])
                
                # å¤„ç†äººå·¥ä¸­æ–­èŠ‚ç‚¹ï¼šæ ¸å¿ƒä¿®æ”¹ç‚¹
                if source == "__interrupt__":
                    interrupts.extend(update)
                    # è°ƒç”¨å®¡æ‰¹å‡½æ•°ï¼Œè·å–å®¡æ‰¹ç»“æœ
                    approval_granted = _handle_interrupt(update[0])
                    # è‹¥æ‹’ç»ï¼Œç›´æ¥ç»ˆæ­¢å¾ªç¯
                    if not approval_granted:
                        break

    # æµç¨‹ç»“æŸæç¤º
    print("\n===== æ‰§è¡Œæµç¨‹ç»“æŸ =====")



# 5.3 Streaming from sub-agents
from typing import Any

from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.messages import AIMessage, AnyMessage


def get_weather(city: str) -> str:
    """Get weather for a given city."""

    return f"It's always sunny in {city}!"


weather_agent = create_agent(
    model=model,
    tools=[get_weather],
    name="weather_agent",  
)


def call_weather_agent(query: str) -> str:
    """Query the weather agent."""
    result = weather_agent.invoke({
        "messages": [{"role": "user", "content": query}]
    })
    return result["messages"][-1].text


from copy import deepcopy

supervisor_model = deepcoy(model)

agent = create_agent(
    model=supervisor_model,
    tools=[call_weather_agent],
    name="supervisor",  
)


def _render_message_chunk(token: AIMessageChunk) -> None:
    if token.text:
        print(token.text, end="|")
    if token.tool_call_chunks:
        print(token.tool_call_chunks)


def _render_completed_message(message: AnyMessage) -> None:
    if isinstance(message, AIMessage) and message.tool_calls:
        print(f"Tool calls: {message.tool_calls}")
    if isinstance(message, ToolMessage):
        print(f"Tool response: {message.content_blocks}")


input_message = {"role": "user", "content": "What is the weather in Boston?"}
current_agent = None
for _, stream_mode, data in agent.stream(
    {"messages": [input_message]},
    stream_mode=["messages", "updates"],
    subgraphs=True,  
):
    if stream_mode == "messages":
        token, metadata = data
        if agent_name := metadata.get("lc_agent_name"):  
            if agent_name != current_agent:  
                print(f"ğŸ¤– {agent_name}: ")  
                current_agent = agent_name  
        if isinstance(token, AIMessage):
            _render_message_chunk(token)
    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):
                _render_completed_message(update["messages"][-1])


# 5.4 Disable streaming
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4o",
    streaming=False
)

